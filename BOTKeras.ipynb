{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BOTKeras.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m8uBlwaRAWNQ"},"outputs":[],"source":[""]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","drive.mount(\"/content/drive/\")\n","\n","mypath = \"drive/My Drive/CS598DLHProject\"\n","os.listdir(mypath)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ls8ZFOxKUveG","executionInfo":{"status":"ok","timestamp":1650498067145,"user_tz":420,"elapsed":1143,"user":{"displayName":"Nisarg Bipinchandra Mistry","userId":"09780882934527034472"}},"outputId":"62e4682b-60a0-4bec-c32b-95af353ba213"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["['data', 'BOTKeras.ipynb']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import gzip\n","import pickle as pkl\n","\n","\n","DATA_DIR = mypath + \"/data/\"\n","\n","X_NOTES_INDEXED_FILE = f'{DATA_DIR}X_NOTES_INDEXED.gz'\n","Y_ICD9_FILE = f'{DATA_DIR}Y_ICD9.gz'\n","Y_ICD9_ROLLED_FILE = f'{DATA_DIR}Y_ICD9_ROLLED.gz'\n","\n","# X dataset\n","if os.path.exists(X_NOTES_INDEXED_FILE):\n","  print ('reading from saved file X_NOTES_INDEXED_FILE: ', X_NOTES_INDEXED_FILE)\n","  with gzip.open(X_NOTES_INDEXED_FILE, \"rb\") as f:\n","      X = pkl.load(f)\n","  print('X.type: ',type(X))\n","  print('X.shape: ', X.shape)\n","# else:\n","#   # save data and label to file\n","#   with gzip.open(X_NOTES_INDEXED_FILE, \"wb\") as f:\n","#       pkl.dump(X, f)\n","#   print('X saved')\n","\n","# Y_ICD9 dataset\n","if os.path.exists(Y_ICD9_FILE):\n","  print ('reading from saved file Y_ICD9_FILE: ', Y_ICD9_FILE)\n","  with gzip.open(Y_ICD9_FILE, \"rb\") as f:\n","      Y_ICD9 = pkl.load(f)\n","  print('Y_ICD9.type: ',type(Y_ICD9))\n","  print('Y_ICD9.shape: ', Y_ICD9.shape)\n","# else:\n","#   # save data and label to file\n","#   with gzip.open(Y_ICD9_FILE, \"wb\") as f:\n","#       pkl.dump(Y_ICD9, f)\n","#   print('Y_ICD9 saved')\n","\n","# Y_ICD9_ROLLED dataset\n","if os.path.exists(Y_ICD9_ROLLED_FILE):\n","  print ('reading from saved file Y_ICD9_ROLLED_FILE: ', Y_ICD9_ROLLED_FILE)\n","  with gzip.open(Y_ICD9_ROLLED_FILE, \"rb\") as f:\n","      Y_ICD9_ROLLED = pkl.load(f)\n","  print('Y_ICD9_ROLLED.type: ',type(Y_ICD9_ROLLED))\n","  print('Y_ICD9_ROLLED.shape: ', Y_ICD9_ROLLED.shape)\n","# else:\n","#   # save data and label to file\n","#   with gzip.open(Y_ICD9_ROLLED_FILE, \"wb\") as f:\n","#       pkl.dump(Y_ICD9_ROLLED, f)\n","#   print('Y_ICD9_ROLLED saved')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pu4EVTKdW2c2","outputId":"d76e701f-c571-4fd0-dbf9-f0a052961616"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["reading from saved file X_NOTES_INDEXED_FILE:  drive/My Drive/CS598DLHProject/data/X_NOTES_INDEXED.gz\n","X.type:  <class 'numpy.ndarray'>\n","X.shape:  (399631, 2200)\n","reading from saved file Y_ICD9_FILE:  drive/My Drive/CS598DLHProject/data/Y_ICD9.gz\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing import sequence"],"metadata":{"id":"MvNyAbGBAq2U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ngram_range = 1\n","max_features = 5000\n","maxlen = 400\n","batch_size = 32\n","embedding_dims = 50\n","epochs = 1\n","\n","print('Loading data...')\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(x_train), 'train sequences')\n","print(len(x_test), 'test sequences')\n","print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n","print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kuGCBHUBHrM","executionInfo":{"status":"ok","timestamp":1649905354238,"user_tz":420,"elapsed":4969,"user":{"displayName":"Nisarg Mistry","userId":"07770741063168990418"}},"outputId":"a824ad30-eeea-4975-c2ca-aafb03af4b0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","17473536/17464789 [==============================] - 0s 0us/step\n","25000 train sequences\n","25000 test sequences\n","Average train sequence length: 238\n","Average test sequence length: 230\n"]}]},{"cell_type":"code","source":["def create_ngram_set(input_list, ngram_value=2):\n","    \"\"\"\n","    Extract a set of n-grams from a list of integers.\n","    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n","    {(4, 9), (4, 1), (1, 4), (9, 4)}\n","    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n","    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n","    \"\"\"\n","    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n","\n","\n","def add_ngram(sequences, token_indice, ngram_range=2):\n","    \"\"\"\n","    Augment the input list of list (sequences) by appending n-grams values.\n","    Example: adding bi-gram\n","    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n","    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n","    # >>> add_ngram(sequences, token_indice, ngram_range=2)\n","    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n","    Example: adding tri-gram\n","    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n","    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n","    # >>> add_ngram(sequences, token_indice, ngram_range=3)\n","    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n","    \"\"\"\n","    new_sequences = []\n","    for input_list in sequences:\n","        new_list = input_list[:]\n","        for ngram_value in range(2, ngram_range + 1):\n","            for i in range(len(new_list) - ngram_value + 1):\n","                ngram = tuple(new_list[i:i + ngram_value])\n","                if ngram in token_indice:\n","                    new_list.append(token_indice[ngram])\n","        new_sequences.append(new_list)\n","\n","    return new_sequences"],"metadata":{"id":"uFZAM3pJA6kD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import Model\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n","\n","class FastText(Model):\n","\n","    def __init__(self,\n","                 maxlen,\n","                 max_features,\n","                 embedding_dims,\n","                 class_num=1,\n","                 last_activation='sigmoid'):\n","        super(FastText, self).__init__()\n","        self.maxlen = maxlen\n","        self.max_features = max_features\n","        self.embedding_dims = embedding_dims\n","        self.class_num = class_num\n","        self.last_activation = last_activation\n","        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n","        self.avg_pooling = GlobalAveragePooling1D()\n","        self.classifier = Dense(self.class_num, activation=self.last_activation)\n","\n","    def call(self, inputs):\n","        if len(inputs.get_shape()) != 2:\n","            raise ValueError('The rank of inputs of FastText must be 2, but now is %d' % len(inputs.get_shape()))\n","        if inputs.get_shape()[1] != self.maxlen:\n","            raise ValueError('The maxlen of inputs of FastText must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n","        embedding = self.embedding(inputs)\n","        x = self.avg_pooling(embedding)\n","        output = self.classifier(x)\n","        return output"],"metadata":{"id":"2UrsOzCVBMKv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN model"],"metadata":{"id":"IuQgcuK7X7JI"}},{"cell_type":"code","source":["from tensorflow.keras import Model\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Conv1D, GlobalMaxPool1D\n","\n","class CNN(Model):\n","\n","    def __init__(self,\n","                 maxlen,\n","                 max_features,\n","                 embedding_dims,\n","                 class_num=1,\n","                 last_activation='relu'):\n","        super(CNN, self).__init__()\n","        self.maxlen = maxlen\n","        self.max_features = max_features\n","        self.embedding_dims = embedding_dims\n","        self.class_num = class_num\n","        self.last_activation = last_activation\n","        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n","        self.conv = Conv1D(250, 3, activation=last_activation)\n","        self.max_pooling = GlobalMaxPool1D()\n","        self.classifier = Dense(self.class_num, activation=self.last_activation)\n","\n","    def call(self, inputs):\n","        if len(inputs.get_shape()) != 2:\n","            raise ValueError('The rank of inputs of FastText must be 2, but now is %d' % len(inputs.get_shape()))\n","        if inputs.get_shape()[1] != self.maxlen:\n","            raise ValueError('The maxlen of inputs of FastText must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n","        embedding = self.embedding(inputs)\n","        conv = self.conv(embedding)\n","        x = self.max_pooling(conv)\n","        output = self.classifier(x)\n","        return output"],"metadata":{"id":"ztmrFEl6Com9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN 3 Layer model"],"metadata":{"id":"fMJLeHZJX-w6"}},{"cell_type":"code","source":["from tensorflow.keras import Model\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Conv1D, GlobalMaxPool1D, concatenate\n","\n","class CNN3Layer(Model):\n","\n","    def __init__(self,\n","                 maxlen,\n","                 max_features,\n","                 embedding_dims,\n","                 class_num=1,\n","                 last_activation='relu'):\n","        super(CNN3Layer, self).__init__()\n","        self.maxlen = maxlen\n","        self.max_features = max_features\n","        self.embedding_dims = embedding_dims\n","        self.class_num = class_num\n","        self.last_activation = last_activation\n","        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n","        self.conv1 = Conv1D(250, 2, activation=last_activation)\n","        self.conv2 = Conv1D(250, 3, activation=last_activation)\n","        self.conv3 = Conv1D(250, 4, activation=last_activation)\n","\n","        self.max_pooling1 = GlobalMaxPool1D()\n","        self.max_pooling2 = GlobalMaxPool1D()\n","        self.max_pooling3 = GlobalMaxPool1D()\n","\n","        self.classifier = Dense(self.class_num, activation=self.last_activation)\n","\n","    def call(self, inputs):\n","        if len(inputs.get_shape()) != 2:\n","            raise ValueError('The rank of inputs of FastText must be 2, but now is %d' % len(inputs.get_shape()))\n","        if inputs.get_shape()[1] != self.maxlen:\n","            raise ValueError('The maxlen of inputs of FastText must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n","        embedding = self.embedding(inputs)\n","        conv1 = self.conv1(embedding)\n","        conv2 = self.conv2(embedding)\n","        conv3 = self.conv3(embedding)\n","\n","        x1 = self.max_pooling1(conv1)\n","        x2 = self.max_pooling2(conv2)\n","        x3 = self.max_pooling3(conv3)\n","        \n","        x = concatenate([x1, x2, x3])\n","        output = self.classifier(x)\n","        return output"],"metadata":{"id":"Qs5lRvsMEFza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","if ngram_range > 1:\n","    print('Adding {}-gram features'.format(ngram_range))\n","    # Create set of unique n-gram from the training set.\n","    ngram_set = set()\n","    for input_list in x_train:\n","        for i in range(2, ngram_range + 1):\n","            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n","            ngram_set.update(set_of_ngram)\n","\n","    # Dictionary mapping n-gram token to a unique integer.\n","    # Integer values are greater than max_features in order\n","    # to avoid collision with existing features.\n","    start_index = max_features + 1\n","    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n","    indice_token = {token_indice[k]: k for k in token_indice}\n","\n","    # max_features is the highest integer that could be found in the dataset.\n","    max_features = np.max(list(indice_token.keys())) + 1\n","\n","    # Augmenting x_train and x_test with n-grams features\n","    x_train = add_ngram(x_train, token_indice, ngram_range)\n","    x_test = add_ngram(x_test, token_indice, ngram_range)\n","    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n","    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n","\n","print('Pad sequences (samples x time)...')\n","x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)\n","\n","print('Build model...')\n","# model = FastText(maxlen, max_features, embedding_dims)\n","\n","model = CNN3Layer(maxlen, max_features, embedding_dims)\n","\n","model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n","\n","print('Train...')\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          callbacks=[early_stopping],\n","          validation_data=(x_test, y_test))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0XT6cV-A7dv","executionInfo":{"status":"ok","timestamp":1649906970489,"user_tz":420,"elapsed":263242,"user":{"displayName":"Nisarg Mistry","userId":"07770741063168990418"}},"outputId":"b14a5c16-5bd6-4dbd-9b09-08d359b3bfca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pad sequences (samples x time)...\n","x_train shape: (25000, 400)\n","x_test shape: (25000, 400)\n","Build model...\n","Train...\n","782/782 [==============================] - 244s 311ms/step - loss: 0.4476 - accuracy: 0.8118 - val_loss: 0.3805 - val_accuracy: 0.8683\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fa87d203310>"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["print('Test...')\n","result = model.predict(x_test)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4NiBTUu2BV6S","executionInfo":{"status":"ok","timestamp":1649907052654,"user_tz":420,"elapsed":82182,"user":{"displayName":"Nisarg Mistry","userId":"07770741063168990418"}},"outputId":"8a23de83-0f8f-409c-9456-786aa038e569"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test...\n","[[0.        ]\n"," [1.1173838 ]\n"," [0.53399014]\n"," ...\n"," [0.        ]\n"," [0.11631893]\n"," [0.17010495]]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"W3l4idkvCPa_"},"execution_count":null,"outputs":[]}]}