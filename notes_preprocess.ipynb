{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import Levenshtein as lev\n",
    "from collections import Counter\n",
    "import os.path\n",
    "\n",
    "\n",
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to remove the punctuation, except Apostrophe\n",
    "\n",
    "mypunctuation = string.punctuation.replace(\"'\", \"\")\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in mypunctuation:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14222,)\n",
      "0    100001\n",
      "1    100009\n",
      "Name: HADM_ID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unique HADM IDs with Diabetes\n",
    "df_hdm_id_diabetes = pkl.load(open(f'{DATA_DIR}diag_diabetes_hadm_ids.p','rb'))\n",
    "\n",
    "print(df_hdm_id_diabetes.shape)\n",
    "print(df_hdm_id_diabetes.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2083180, 5)\n",
      "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
      "0       22532  167853.0  Discharge summary      Report   \n",
      "1       13702  107527.0  Discharge summary      Report   \n",
      "\n",
      "                                                TEXT  \n",
      "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
      "1  Admission Date:  [**2118-6-2**]       Discharg...  \n"
     ]
    }
   ],
   "source": [
    "df_notes = (pd.read_csv(f'{DATA_DIR}NOTEEVENTS.csv.gz', low_memory=False)\n",
    "                [['SUBJECT_ID','HADM_ID','CATEGORY', 'DESCRIPTION', 'TEXT']])\n",
    "print(df_notes.shape)\n",
    "print(df_notes.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406203, 5)\n",
      "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
      "0       28063  121936.0  Discharge summary      Report   \n",
      "1       28063  121936.0               Echo      Report   \n",
      "\n",
      "                                                TEXT  \n",
      "0  Admission Date:  [**2125-2-9**]              D...  \n",
      "1  PATIENT/TEST INFORMATION:\\nIndication: Aortic ...  \n"
     ]
    }
   ],
   "source": [
    "df_notes_diabetes = pd.merge(df_notes, df_hdm_id_diabetes, on=['HADM_ID'], how='inner')\n",
    "\n",
    "print(df_notes_diabetes.shape)\n",
    "print(df_notes_diabetes.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Admission Date      dddd d d                 D...\n",
       "1    PATIENT TEST INFORMATION \\nIndication  Aortic ...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_diabetes['TEXT'] = df_notes_diabetes['TEXT'].apply(remove_punctuations).replace('[\\d]', 'd',regex=True)\n",
    "df_notes_diabetes['TEXT'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28063</td>\n",
       "      <td>121936.0</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>admission date      dddd d d                 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28063</td>\n",
       "      <td>121936.0</td>\n",
       "      <td>Echo</td>\n",
       "      <td>Report</td>\n",
       "      <td>patient test information \\nindication  aortic ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
       "0       28063  121936.0  Discharge summary      Report   \n",
       "1       28063  121936.0               Echo      Report   \n",
       "\n",
       "                                                TEXT  \n",
       "0  admission date      dddd d d                 d...  \n",
       "1  patient test information \\nindication  aortic ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_diabetes['TEXT'] = df_notes_diabetes['TEXT'].str.lower()\n",
    "\n",
    "df_notes_diabetes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
      "0       28063  121936.0  Discharge summary      Report   \n",
      "1       28063  121936.0               Echo      Report   \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  admission date      dddd d d                 d...   \n",
      "1  patient test information \\nindication  aortic ...   \n",
      "\n",
      "                                              TOKENS  \n",
      "0  [admission, date, dddd, d, d, discharge, date,...  \n",
      "1  [patient, test, information, indication, aorti...  \n"
     ]
    }
   ],
   "source": [
    "df_notes_diabetes['TOKENS'] = df_notes_diabetes['TEXT'].str.split()\n",
    "print(df_notes_diabetes.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_freq_dict_list = []\n",
    "\n",
    "for idx, tokens in enumerate(df_notes_diabetes['TOKENS']):\n",
    "  word_freq_dict_list.append(dict(Counter(tokens)))\n",
    "\n",
    "print(len(word_freq_dict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303741 80749 20849 851 13\n",
      "98536\n",
      "135874\n",
      "159676\n",
      "162715\n"
     ]
    }
   ],
   "source": [
    "# merge list of word frequency dictionaries created for each notes - to create overall word frequencies\n",
    "\n",
    "# function to merge only dictionaries from dictionary list whose lenth is between min_len and max_len\n",
    "def createWordCorpus(dictList, min_len, max_len, wordCorpus):\n",
    "  for idx, dict1 in enumerate(dictList):\n",
    "    lent = len(dict1)\n",
    "    if lent > min_len and lent <= max_len:\n",
    "      for key, value in dict1.items():\n",
    "        if key in wordCorpus:\n",
    "          wordCorpus[key] = wordCorpus[key] + dict1[key]\n",
    "        else:\n",
    "          wordCorpus[key] = dict1[key]\n",
    "  return wordCorpus\n",
    "\n",
    "# check stats for dictionary length\n",
    "len_200 = 0\n",
    "len_500 = 0\n",
    "len_1000 = 0\n",
    "len_2000 = 0\n",
    "len_rest = 0\n",
    "\n",
    "for dict1 in word_freq_dict_list:\n",
    "  lent = len(dict1)\n",
    "  if lent > 0 and lent <= 200:\n",
    "    len_200 = len_200 + 1\n",
    "  elif lent > 200 and lent <= 500:\n",
    "    len_500 = len_500 + 1\n",
    "  elif lent > 500 and lent <= 1000:\n",
    "    len_1000 = len_1000 + 1\n",
    "  elif lent > 1000 and lent <= 2000:\n",
    "    len_2000 = len_2000 + 1\n",
    "  else:\n",
    "    len_rest = len_rest + 1\n",
    "\n",
    "print(len_200, len_500, len_1000, len_2000, len_rest)\n",
    "\n",
    "\n",
    "wordCorpus = {}\n",
    "\n",
    "wordCorpus = createWordCorpus(word_freq_dict_list, 0, 200, wordCorpus)\n",
    "print(len(wordCorpus))\n",
    "\n",
    "wordCorpus = createWordCorpus(word_freq_dict_list, 200, 500, wordCorpus)\n",
    "print(len(wordCorpus))\n",
    "\n",
    "wordCorpus = createWordCorpus(word_freq_dict_list, 500, 1000, wordCorpus)\n",
    "print(len(wordCorpus))\n",
    "\n",
    "wordCorpus = createWordCorpus(word_freq_dict_list, 1000, 100000, wordCorpus)\n",
    "print(len(wordCorpus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162715\n"
     ]
    }
   ],
   "source": [
    "# print(len(wordCorpus))\n",
    "\n",
    "# write to file - word corpus with frequency - overall\n",
    "# pkl.dump( wordCorpus, open( \"../data/wordCorpus.p\", \"wb\" ) )\n",
    "\n",
    "# read from file - word corpus with frequency - overall\n",
    "\n",
    "wordCorpus = pkl.load(open( \"../data/wordCorpus.p\",'rb'))\n",
    "print(len(wordCorpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109486 53229\n"
     ]
    }
   ],
   "source": [
    "tokens_freq_lt_5 = {}\n",
    "tokens_freq_gt_5 = {}\n",
    "\n",
    "freq_limit = 5\n",
    "for key, value in wordCorpus.items():\n",
    "  if value < freq_limit:\n",
    "    tokens_freq_lt_5[key] = value\n",
    "  else:\n",
    "    tokens_freq_gt_5[key] = value\n",
    "\n",
    "print(len(tokens_freq_lt_5), len(tokens_freq_gt_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find closest match for a word in a word list using Levenshtein disctance\n",
    "def closestMatch(candidateToken, wordList):\n",
    "  min_dist = 99\n",
    "  similar_word = ''\n",
    "  for word in wordList:\n",
    "    dist = lev.distance(candidateToken, word)\n",
    "    if dist <= min_dist:\n",
    "      min_dist = dist\n",
    "      similar_word = word\n",
    "  return candidateToken, similar_word, min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inferolaterl inferolateral 1\n",
      "10000 andiv endive 2\n",
      "20000 bedsie bedsdie 1\n",
      "30000 nimodidpine nimodipine 1\n",
      "40000 activates activites 1\n",
      "50000 coccyl coccy 1\n",
      "60000 reconstutited reconstituted 2\n",
      "70000 stroides stroide 1\n",
      "80000 prmpted prompted 1\n",
      "90000 corroberated corroborated 1\n",
      "100000 hyperternsion hypertension 1\n"
     ]
    }
   ],
   "source": [
    "# token mapping -> generate token mapping for presumbly misspelt word\n",
    "\n",
    "token_map = {}\n",
    "\n",
    "ii = 0\n",
    "for ct in tokens_freq_lt_5:\n",
    "  candidateToken, similar_word, min_dist = closestMatch(ct, tokens_freq_gt_5)\n",
    "  token_map[ct] = similar_word\n",
    "  if ii % 10000 == 0:\n",
    "    print(ii, candidateToken, similar_word, min_dist)\n",
    "  ii = ii + 1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109486\n",
      "109486\n",
      "exacerbation\n"
     ]
    }
   ],
   "source": [
    "print(len(token_map))\n",
    "\n",
    "# write to file - word corpus with frequency - overall\n",
    "pkl.dump(token_map, open( \"../data/word_token_map.p\", \"wb\" ) )\n",
    "\n",
    "# read from file - word corpus with frequency - overall\n",
    "\n",
    "token_map = pkl.load(open( \"../data/word_token_map.p\",'rb'))\n",
    "print(len(token_map))\n",
    "print(token_map['exacerbatiopn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens(tokens):\n",
    "  n_tokens = []\n",
    "  for token in tokens:\n",
    "    if token in tokens_freq_lt_5:\n",
    "      n_tokens.append(token_map[token])\n",
    "    else:\n",
    "      n_tokens.append(token)\n",
    "  return n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [admission, date, dddd, d, d, discharge, date,...\n",
      "1    [patient, test, information, indication, aorti...\n",
      "2    [patient, test, information, indication, aorti...\n",
      "3    [sinus, rhythm, frequent, atrial, premature, b...\n",
      "4    [rhythm, is, most, likely, sinus, rhythm, with...\n",
      "Name: NTOKENS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_notes_diabetes['NTOKENS'] = df_notes_diabetes['TOKENS'].apply(map_tokens)\n",
    "print(df_notes_diabetes['NTOKENS'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate no of tokens for each report\n",
    "df_notes_diabetes['NTOKENS_LEN'] = df_notes_diabetes['NTOKENS'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399631, 8)\n",
      "(6572, 8)\n",
      "   SUBJECT_ID   HADM_ID CATEGORY DESCRIPTION  \\\n",
      "0       28063  121936.0     Echo      Report   \n",
      "1       28063  121936.0     Echo      Report   \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  patient test information \\nindication  aortic ...   \n",
      "1  patient test information \\nindication   aortic...   \n",
      "\n",
      "                                              TOKENS  \\\n",
      "0  [patient, test, information, indication, aorti...   \n",
      "1  [patient, test, information, indication, aorti...   \n",
      "\n",
      "                                             NTOKENS  NTOKENS_LEN  \n",
      "0  [patient, test, information, indication, aorti...          428  \n",
      "1  [patient, test, information, indication, aorti...          306  \n"
     ]
    }
   ],
   "source": [
    "# Filtering based on # tokens - GT 9 and LE 2200\n",
    "df_notes_diabetes_final = df_notes_diabetes[(df_notes_diabetes['NTOKENS_LEN'] > 9) & (df_notes_diabetes['NTOKENS_LEN'] <= 2200)].reset_index(drop=True)\n",
    "print(df_notes_diabetes_final.shape)\n",
    "\n",
    "df_notes_diabetes_filtered = df_notes_diabetes[(df_notes_diabetes['NTOKENS_LEN'] <= 9) | (df_notes_diabetes['NTOKENS_LEN'] > 2200)].reset_index(drop=True)\n",
    "print(df_notes_diabetes_filtered.shape)\n",
    "\n",
    "print(df_notes_diabetes_final.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading saved notes_final_file\n"
     ]
    }
   ],
   "source": [
    "notes_final_file = f'{DATA_DIR}notes_final.csv.gz'\n",
    "\n",
    "if os.path.exists(notes_final_file):\n",
    "  # read saved notes_final\n",
    "  print('reading saved notes_final_file')\n",
    "  df_notes_diabetes_final = (pd.read_csv(f'{DATA_DIR}notes_final.csv.gz', low_memory=False))\n",
    "else:\n",
    "  # write to file\n",
    "  df_notes_diabetes_final.to_csv(f'{DATA_DIR}notes_final.csv.gz', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_tokens_list_file = f'{DATA_DIR}notes_tokens_list.p'\n",
    "\n",
    "if not os.path.exists(notes_final_file):\n",
    "  # write to file\n",
    "  pkl.dump( tokens_freq_gt_5, open(notes_tokens_list_file, \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d114184c886ba29bffe38c6c6fdadc7ffef7ccc1b8a3a158ab752daf223c4cdc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cs598')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
