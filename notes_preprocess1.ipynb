{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import Levenshtein as lev\n",
    "from collections import Counter\n",
    "import os.path\n",
    "import gzip\n",
    "\n",
    "\n",
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to remove the punctuation, except Apostrophe\n",
    "\n",
    "mypunctuation = string.punctuation.replace(\"'\", \"\")\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in mypunctuation:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14222,)\n",
      "0    100001\n",
      "1    100009\n",
      "Name: HADM_ID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unique HADM IDs with Diabetes\n",
    "df_hdm_id_diabetes = pkl.load(open(f'{DATA_DIR}diag_diabetes_hadm_ids.p','rb'))\n",
    "\n",
    "print(df_hdm_id_diabetes.shape)\n",
    "print(df_hdm_id_diabetes.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2083180, 5)\n",
      "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
      "0       22532  167853.0  Discharge summary      Report   \n",
      "1       13702  107527.0  Discharge summary      Report   \n",
      "\n",
      "                                                TEXT  \n",
      "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
      "1  Admission Date:  [**2118-6-2**]       Discharg...  \n"
     ]
    }
   ],
   "source": [
    "df_notes = (pd.read_csv(f'{DATA_DIR}NOTEEVENTS.csv.gz', low_memory=False)\n",
    "                [['SUBJECT_ID','HADM_ID','CATEGORY', 'DESCRIPTION', 'TEXT']])\n",
    "print(df_notes.shape)\n",
    "print(df_notes.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406203, 5)\n",
      "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
      "0       28063  121936.0  Discharge summary      Report   \n",
      "1       28063  121936.0               Echo      Report   \n",
      "\n",
      "                                                TEXT  \n",
      "0  Admission Date:  [**2125-2-9**]              D...  \n",
      "1  PATIENT/TEST INFORMATION:\\nIndication: Aortic ...  \n"
     ]
    }
   ],
   "source": [
    "df_notes_diabetes = pd.merge(df_notes, df_hdm_id_diabetes, on=['HADM_ID'], how='inner')\n",
    "\n",
    "print(df_notes_diabetes.shape)\n",
    "print(df_notes_diabetes.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    admission date      dddd d d                 d...\n",
       "1    patient test information \\nindication  aortic ...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_diabetes['TEXT'] = df_notes_diabetes['TEXT'].apply(remove_punctuations).replace('[\\d]', 'd',regex=True)\n",
    "df_notes_diabetes['TEXT'] = df_notes_diabetes['TEXT'].str.lower()\n",
    "\n",
    "df_notes_diabetes['TEXT'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SUBJECT_ID   HADM_ID           CATEGORY DESCRIPTION  \\\n",
      "0       28063  121936.0  Discharge summary      Report   \n",
      "1       28063  121936.0               Echo      Report   \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  admission date      dddd d d                 d...   \n",
      "1  patient test information \\nindication  aortic ...   \n",
      "\n",
      "                                              TOKENS  \n",
      "0  [admission, date, dddd, d, d, discharge, date,...  \n",
      "1  [patient, test, information, indication, aorti...  \n"
     ]
    }
   ],
   "source": [
    "df_notes_diabetes['TOKENS'] = df_notes_diabetes['TEXT'].str.split()\n",
    "print(df_notes_diabetes.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406203\n"
     ]
    }
   ],
   "source": [
    "word_freq_dict_list = []\n",
    "\n",
    "for idx, tokens in enumerate(df_notes_diabetes['TOKENS']):\n",
    "  word_freq_dict_list.append(dict(Counter(tokens)))\n",
    "\n",
    "print(len(word_freq_dict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303741 80749 20849 851 13\n",
      "reading from file:  ../data/wordCorpus.p\n",
      "162715\n"
     ]
    }
   ],
   "source": [
    "# merge list of word frequency dictionaries created for each notes - to create overall word frequencies\n",
    "\n",
    "# function to merge only dictionaries from dictionary list whose lenth is between min_len and max_len\n",
    "def createWordCorpus(dictList, min_len, max_len, wordCorpus):\n",
    "  for idx, dict1 in enumerate(dictList):\n",
    "    lent = len(dict1)\n",
    "    if lent > min_len and lent <= max_len:\n",
    "      for key, value in dict1.items():\n",
    "        if key in wordCorpus:\n",
    "          wordCorpus[key] = wordCorpus[key] + dict1[key]\n",
    "        else:\n",
    "          wordCorpus[key] = dict1[key]\n",
    "  return wordCorpus\n",
    "\n",
    "# check stats for dictionary length\n",
    "len_200 = 0\n",
    "len_500 = 0\n",
    "len_1000 = 0\n",
    "len_2000 = 0\n",
    "len_rest = 0\n",
    "\n",
    "for dict1 in word_freq_dict_list:\n",
    "  lent = len(dict1)\n",
    "  if lent > 0 and lent <= 200:\n",
    "    len_200 = len_200 + 1\n",
    "  elif lent > 200 and lent <= 500:\n",
    "    len_500 = len_500 + 1\n",
    "  elif lent > 500 and lent <= 1000:\n",
    "    len_1000 = len_1000 + 1\n",
    "  elif lent > 1000 and lent <= 2000:\n",
    "    len_2000 = len_2000 + 1\n",
    "  else:\n",
    "    len_rest = len_rest + 1\n",
    "\n",
    "print(len_200, len_500, len_1000, len_2000, len_rest)\n",
    "\n",
    "\n",
    "wordCorpus_file = f'{DATA_DIR}wordCorpus.p'\n",
    "\n",
    "if not os.path.exists(wordCorpus_file):\n",
    "  wordCorpus = {}\n",
    "  wordCorpus = createWordCorpus(word_freq_dict_list, 0, 200, wordCorpus)\n",
    "  print(len(wordCorpus))\n",
    "\n",
    "  wordCorpus = createWordCorpus(word_freq_dict_list, 200, 500, wordCorpus)\n",
    "  print(len(wordCorpus))\n",
    "\n",
    "  wordCorpus = createWordCorpus(word_freq_dict_list, 500, 1000, wordCorpus)\n",
    "  print(len(wordCorpus))\n",
    "\n",
    "  wordCorpus = createWordCorpus(word_freq_dict_list, 1000, 100000, wordCorpus)\n",
    "  print(len(wordCorpus))\n",
    "  # write to file - word corpus with frequency - overall\n",
    "  pkl.dump( wordCorpus, open(wordCorpus_file, \"wb\" ) )\n",
    "else:\n",
    "  # read from file - word corpus with frequency - overall\n",
    "  print('reading from file: ', wordCorpus_file)\n",
    "  wordCorpus = pkl.load(open(wordCorpus_file,'rb'))\n",
    "\n",
    "print(len(wordCorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109486 53229\n"
     ]
    }
   ],
   "source": [
    "tokens_freq_lt_5 = {}\n",
    "tokens_freq_gt_5 = {}\n",
    "\n",
    "freq_limit = 5\n",
    "for key, value in wordCorpus.items():\n",
    "  if value < freq_limit:\n",
    "    tokens_freq_lt_5[key] = value\n",
    "  else:\n",
    "    tokens_freq_gt_5[key] = value\n",
    "\n",
    "print(len(tokens_freq_lt_5), len(tokens_freq_gt_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find closest match for a word in a word list using Levenshtein disctance\n",
    "def closestMatch(candidateToken, wordList):\n",
    "  min_dist = 99\n",
    "  similar_word = ''\n",
    "  for word in wordList:\n",
    "    dist = lev.distance(candidateToken, word)\n",
    "    if dist <= min_dist:\n",
    "      min_dist = dist\n",
    "      similar_word = word\n",
    "  return candidateToken, similar_word, min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from  ../data/word_token_map.p\n",
      "109486\n",
      "exacerbation\n"
     ]
    }
   ],
   "source": [
    "# token mapping -> generate token mapping for presumbly misspelt word\n",
    "\n",
    "word_token_map_file = f'{DATA_DIR}word_token_map.p'\n",
    "\n",
    "if not os.path.exists(word_token_map_file):\n",
    "  token_map = {}\n",
    "\n",
    "  ii = 0\n",
    "  for ct in tokens_freq_lt_5:\n",
    "    candidateToken, similar_word, min_dist = closestMatch(ct, tokens_freq_gt_5)\n",
    "    token_map[ct] = similar_word\n",
    "    if ii % 10000 == 0:\n",
    "      print(ii, candidateToken, similar_word, min_dist)\n",
    "    ii = ii + 1\n",
    "  # write to file - word corpus with frequency - overall\n",
    "  pkl.dump(token_map, open(word_token_map_file, \"wb\" ) )\n",
    "else:\n",
    "  print('reading from ', word_token_map_file)\n",
    "  # read from file - word corpus with frequency - overall\n",
    "  token_map = pkl.load(open(word_token_map_file,'rb'))\n",
    "\n",
    "print(len(token_map))\n",
    "print(token_map['exacerbatiopn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens(tokens):\n",
    "  n_tokens = []\n",
    "  for token in tokens:\n",
    "    if token in tokens_freq_lt_5:\n",
    "      n_tokens.append(token_map[token])\n",
    "    else:\n",
    "      n_tokens.append(token)\n",
    "  return n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [admission, date, dddd, d, d, discharge, date,...\n",
      "1    [patient, test, information, indication, aorti...\n",
      "2    [patient, test, information, indication, aorti...\n",
      "3    [sinus, rhythm, frequent, atrial, premature, b...\n",
      "4    [rhythm, is, most, likely, sinus, rhythm, with...\n",
      "Name: NTOKENS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_notes_diabetes['NTOKENS'] = df_notes_diabetes['TOKENS'].apply(map_tokens)\n",
    "print(df_notes_diabetes['NTOKENS'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate no of tokens for each report\n",
    "df_notes_diabetes['NTOKENS_LEN'] = df_notes_diabetes['NTOKENS'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399623, 8)\n",
      "(6580, 8)\n",
      "   SUBJECT_ID   HADM_ID CATEGORY DESCRIPTION  \\\n",
      "0       28063  121936.0     Echo      Report   \n",
      "1       28063  121936.0     Echo      Report   \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  patient test information \\nindication  aortic ...   \n",
      "1  patient test information \\nindication   aortic...   \n",
      "\n",
      "                                              TOKENS  \\\n",
      "0  [patient, test, information, indication, aorti...   \n",
      "1  [patient, test, information, indication, aorti...   \n",
      "\n",
      "                                             NTOKENS  NTOKENS_LEN  \n",
      "0  [patient, test, information, indication, aorti...          428  \n",
      "1  [patient, test, information, indication, aorti...          306  \n"
     ]
    }
   ],
   "source": [
    "# Filtering based on # tokens - GT 9 and LE 2200\n",
    "df_notes_diabetes_final = df_notes_diabetes[(df_notes_diabetes['NTOKENS_LEN'] > 9) & (df_notes_diabetes['NTOKENS_LEN'] < 2200)].reset_index(drop=True)\n",
    "print(df_notes_diabetes_final.shape)\n",
    "\n",
    "df_notes_diabetes_filtered = df_notes_diabetes[(df_notes_diabetes['NTOKENS_LEN'] <= 9) | (df_notes_diabetes['NTOKENS_LEN'] >= 2200)].reset_index(drop=True)\n",
    "print(df_notes_diabetes_filtered.shape)\n",
    "\n",
    "print(df_notes_diabetes_final.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399623, 7)\n"
     ]
    }
   ],
   "source": [
    "df_notes_diabetes_final = df_notes_diabetes_final.drop(columns=['TOKENS'])\n",
    "print(df_notes_diabetes_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_notes_diabetes_final['NTOKENS'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "notes_final_file = f'{DATA_DIR}notes_final.gz'\n",
    "\n",
    "if os.path.exists(notes_final_file):\n",
    "  # read saved notes_final\n",
    "  print('reading saved notes_final_file: ', notes_final_file)\n",
    "  with gzip.open(notes_final_file, \"rb\") as f:\n",
    "      df_notes_diabetes_final = pkl.load(f)\n",
    "else:\n",
    "  # write to file\n",
    "  with gzip.open(notes_final_file, \"wb\") as f:\n",
    "      pkl.dump(df_notes_diabetes_final, f)\n",
    "\n",
    "print(type(df_notes_diabetes_final['NTOKENS'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SUBJECT_ID   HADM_ID CATEGORY DESCRIPTION  \\\n",
      "0       28063  121936.0     Echo      Report   \n",
      "1       28063  121936.0     Echo      Report   \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  patient test information \\nindication  aortic ...   \n",
      "1  patient test information \\nindication   aortic...   \n",
      "\n",
      "                                             NTOKENS  NTOKENS_LEN  \n",
      "0  [patient, test, information, indication, aorti...          428  \n",
      "1  [patient, test, information, indication, aorti...          306  \n"
     ]
    }
   ],
   "source": [
    "print(df_notes_diabetes_final.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_notes_diabetes_final['NTOKENS'][0]) == df_notes_diabetes_final['NTOKENS_LEN'][0], 'length of 1st sequences does not match, incorrect data'\n",
    "assert len(df_notes_diabetes_final['NTOKENS'][1]) == df_notes_diabetes_final['NTOKENS_LEN'][1], 'length of 2nd sequences does not match, incorrect data'\n",
    "assert len(df_notes_diabetes_final['NTOKENS'][2]) == df_notes_diabetes_final['NTOKENS_LEN'][2], 'length of 3rd sequences does not match, incorrect data'\n",
    "assert len(df_notes_diabetes_final['NTOKENS'][399622]) == df_notes_diabetes_final['NTOKENS_LEN'][399622], 'length of 399622 sequences does not match, incorrect data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_freq_gt_5 len:  53229\n"
     ]
    }
   ],
   "source": [
    "notes_tokens_list_file = f'{DATA_DIR}notes_tokens_list.p'\n",
    "\n",
    "if not os.path.exists(notes_tokens_list_file):\n",
    "  # write to file\n",
    "  print('tokens_freq_gt_5 len: ', tokens_freq_gt_5)\n",
    "  pkl.dump( tokens_freq_gt_5, open(notes_tokens_list_file, \"wb\" ) )\n",
    "else:\n",
    "  tokens_freq_gt_5 = pkl.load(open(notes_tokens_list_file,'rb'))\n",
    "  print('tokens_freq_gt_5 len: ', len(tokens_freq_gt_5))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d114184c886ba29bffe38c6c6fdadc7ffef7ccc1b8a3a158ab752daf223c4cdc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cs598')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
