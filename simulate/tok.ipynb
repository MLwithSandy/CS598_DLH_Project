{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "import string\n",
    "import gzip\n",
    "\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORD_OCCURENCES = 5\n",
    "MIN_SEQ_LEN = 9\n",
    "MAX_SEQ_LEN = 2200\n",
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_diag_diabetes_hadm_ids.shape:    (14222,)\n",
      "len diag_diabetes_hadm_ids_dict:  14222\n"
     ]
    }
   ],
   "source": [
    "df_diag_diabetes_hadm_ids = pickle.load(open(f'{DATA_DIR}diag_diabetes_hadm_ids.p','rb'))\n",
    "print('df_diag_diabetes_hadm_ids.shape:   ', df_diag_diabetes_hadm_ids.shape)\n",
    "diag_diabetes_hadm_ids = df_diag_diabetes_hadm_ids.to_list()\n",
    "\n",
    "diag_diabetes_hadm_ids_dict =  {str(k): v for v, k in enumerate(diag_diabetes_hadm_ids)}\n",
    "\n",
    "print('len diag_diabetes_hadm_ids_dict: ', len(diag_diabetes_hadm_ids_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('100458' in diag_diabetes_hadm_ids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_diagnosis = {}\n",
    "rolled_diagnosis = {}\n",
    "regular_icd9_lookup = []\n",
    "rolled_icd9_lookup = []\n",
    "\n",
    "# with open('diagnosis.csv', 'rb') as f:\n",
    "with gzip.open(f'{DATA_DIR}DIAGNOSES_ICD.csv.gz', 'rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if row[2] in diag_diabetes_hadm_ids_dict:\n",
    "            note_id = (row[1], row[2])\n",
    "            regular_icd9 = row[4]\n",
    "            rolled_icd9 = regular_icd9[:3]\n",
    "\n",
    "            if regular_icd9 not in regular_icd9_lookup:\n",
    "                regular_icd9_lookup.append(regular_icd9)\n",
    "            if rolled_icd9 not in rolled_icd9_lookup:\n",
    "                rolled_icd9_lookup.append(rolled_icd9)\n",
    "\n",
    "            regular_note_diagnosis = regular_diagnosis.get(note_id, [])\n",
    "            rolled_note_diagnosis = rolled_diagnosis.get(note_id, [])\n",
    "            regular_idx = regular_icd9_lookup.index(regular_icd9)\n",
    "            rolled_idx = rolled_icd9_lookup.index(rolled_icd9)\n",
    "\n",
    "            if regular_idx not in regular_note_diagnosis:\n",
    "                regular_diagnosis[note_id] = regular_note_diagnosis + [regular_idx]\n",
    "            if rolled_idx not in rolled_note_diagnosis:\n",
    "                rolled_diagnosis[note_id] = rolled_note_diagnosis + [rolled_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14222 14222 4103 781\n",
      "14222 14222 4103 781\n"
     ]
    }
   ],
   "source": [
    "print(len(regular_diagnosis), len(rolled_diagnosis), len(regular_icd9_lookup), len(rolled_icd9_lookup))\n",
    "# 58976 58976 6985 943\n",
    "# 14222 14222 4103 781\n",
    "\n",
    "pickle.dump( regular_diagnosis, open( f'{DATA_DIR}regular_diagnosis.pickle', \"wb\" ) )\n",
    "\n",
    "regular_diagnosis = pickle.load(open( f'{DATA_DIR}regular_diagnosis.pickle', \"rb\" ) )\n",
    "\n",
    "print(len(regular_diagnosis), len(rolled_diagnosis), len(regular_icd9_lookup), len(rolled_icd9_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('56174', '189681') Discharge summary ['183', '56174', '189681', '2118-12-09', '', '', 'Discharge summary', 'Report', '', '', 'Admission Date:  [**2118-12-7**]              Discharge Date:   [**2118-12-9**]\\n\\nDate of Birth:  [**2073-12-25**]             Sex:   F\\n\\nService: NEUROSURGERY\\n\\nAllergies:\\nCodeine\\n\\nAttending:[**First Name3 (LF) 1854**]\\nChief Complaint:\\nSkull defect\\n\\nMajor Surgical or Invasive Procedure:\\ns/p cranioplasty on [**2118-12-7**]\\n\\n\\nHistory of Present Illness:\\n44 yo female with a h/o left frontal AVM in the supplementary\\nmotor area. The AVM was treated with stereotactic radiosurgery\\n(Gamma Knife)in [**2114**]. In [**2116**], the patient developed a seizure\\ndisorder. [**2118-5-27**] she developed\\nheadaches and after an MRI and a digital angiogram showed no\\nresidual pathological vessels, a contrast enhancing lesion\\nwith massive focal residual edema was diagnosed- very\\nlikely represents radionecrosis. The patient had midline\\nshift and mass effect. On [**2118-8-10**] she had a left craniotomy for\\nresection of the radionecrosis. She then presented to the office\\nin [**2118-8-27**] with increased left facial swelling and incision\\ndrainage, she was taken to the OR for a wound washout and\\ncraniectomy. She now returns for a cranioplasty after a long\\ncourse of outpatient IV antibiotic therapy.\\n\\n\\nPast Medical History:\\nseizures,h/o radio therapy for avm has resid edema causing\\nseizures; Dysrhythmia (palps w/ panic attacks), Recent Upper\\nRespiratory Infection\\nPalpitations with panic attacks\\nPanic, anxiety\\nDepression\\nh/o nephrolithiasis (at 20yrs old)\\nTB as a child (treated)\\n\\n\\nSocial History:\\nMarried. Lives with husband.\\n\\nFamily History:\\nNon-contributory\\n\\nPhysical Exam:\\nOn admission:\\nAOx3, PERRL, Face symm, tongue midline. EOM intact w/o\\nnystagmus. Speech clear and fluent. Comprehension intact.\\nFollows commands. No pronator. MAE [**5-31**]\\n\\nUpon discharge:\\nAOx3. Neuro intact. MAE [**5-31**]. Incision C/D/I. Ambulating,\\ntolerating POs\\n\\nPertinent Results:\\nCT Head [**2118-12-7**]: (Post-Op)\\nPatient is status post left frontal cranioplasty. Persistent\\nvasogenic edema in the left frontal lobe, unchanged. No shift of\\nnormally\\nmidline structures or acute hemorrhage identified.\\n\\n*******************\\n\\n[**2118-12-7**] 03:13PM   WBC-13.8*# RBC-4.76 HGB-12.8 HCT-37.6 MCV-79*\\nMCH-27.0 MCHC-34.2 RDW-14.4\\n[**2118-12-7**] 03:13PM   PLT COUNT-555*\\n[**2118-12-7**] 03:13PM   CALCIUM-9.2 PHOSPHATE-3.4 MAGNESIUM-2.3\\n[**2118-12-7**] 03:13PM   estGFR-Using this\\n[**2118-12-7**] 03:13PM   GLUCOSE-128* CREAT-0.9 SODIUM-141\\nPOTASSIUM-4.1 CHLORIDE-102 TOTAL CO2-30 ANION GAP-13\\n\\nBrief Hospital Course:\\n44 yo female who was electively admitted for a cranioplasty with\\nDr. [**Last Name (STitle) **]. Immediately post-op she remained in the PACU\\novernight. Overnight she voided 4L and received 1L NS bolus. POD\\n1 she was transferred to the floor. Prior to transfer she was\\nnoted to have increase HR, low BP, and low urine output thus\\nreceived 1L of NS. On the floor, she was OOB to chair,\\ntolerating a regular diet. She was neurologically intact and\\ncleared for discharge on [**2118-12-9**].\\n\\nMedications on Admission:\\nFLUTICASONE [FLONASE] -  (Prescribed by Other Provider) - 50 mcg\\nSpray, Suspension - 2 sprays(s) each nostril daily as needed for\\nnasal congestion\\nLEVETIRACETAM [KEPPRA] - 1,000 mg Tablet - 1 Tablet(s) by mouth\\ntwice a day - No Substitution\\nLEVETIRACETAM [KEPPRA] - 500 mg Tablet - 1 Tablet(s) by mouth at\\nbedtime - No Substitution\\nLEVETIRACETAM [KEPPRA] - 250 mg Tablet - 1 Tablet(s) by mouth\\nfour times a day - No Substitution\\nOSELTAMIVIR [TAMIFLU] - 75 mg Capsule - one Capsule(s) by mouth\\ntwice a day x 5 days\\nVENLAFAXINE - 50 mg Tablet - One Tablet(s) by mouth twice a day\\nACETAMINOPHEN [TYLENOL] -  (OTC) - Dosage uncertain\\nIBUPROFEN [ADVIL MIGRAINE] -  (OTC) - 200 mg Capsule - 1\\nCapsule(s) by mouth once a day as needed for headache\\n\\n\\nDischarge Medications:\\n1. Acetaminophen 325 mg Tablet Sig: 1-2 Tablets PO Q4H (every 4\\nhours) as needed for pain/t>100/HA.\\n2. Bisacodyl 5 mg Tablet, Delayed Release (E.C.) Sig: Two (2)\\nTablet, Delayed Release (E.C.) PO DAILY (Daily) as needed for\\nconstipation.\\n3. Docusate Sodium 100 mg Capsule Sig: One (1) Capsule PO BID (2\\ntimes a day).\\nDisp:*60 Capsule(s)* Refills:*2*\\n4. Hydromorphone 2 mg Tablet Sig: 1-2 Tablets PO Q4H (every 4\\nhours) as needed for pain.\\nDisp:*30 Tablet(s)* Refills:*0*\\n5. Venlafaxine 25 mg Tablet Sig: Two (2) Tablet PO BID (2 times\\na day).\\n6. Fluticasone 50 mcg/Actuation Spray, Suspension Sig: Two (2)\\nSpray Nasal DAILY (Daily) as needed for nasal congestion.\\n7. Dexamethasone 2 mg Tablet Sig: One (1) Tablet PO Q6H (every 6\\nhours) for 6 days: Take 2mg Q6hrs [**Date range (1) 1855**], take 2mg Q12\\n[**Date range (1) 1856**], Take 2mg Q24 [**12-14**], then stop.\\nDisp:*16 Tablet(s)* Refills:*0*\\n8. Levetiracetam 500 mg Tablet Sig: 2.5 Tablets PO BID (2 times\\na day).\\n\\n\\nDischarge Disposition:\\nHome\\n\\nDischarge Diagnosis:\\nSkull defect\\ns/p cranioplasty\\n\\n\\nDischarge Condition:\\nNeurologically Stable\\n\\n\\nDischarge Instructions:\\nGeneral Instructions\\n\\n??????\\tHave a friend/family member check your incision daily for\\nsigns of infection.\\n??????\\tTake your pain medicine as prescribed.\\n??????\\tExercise should be limited to walking; no lifting, straining,\\nor excessive bending.\\n??????\\tYou may wash your hair only after sutures and/or staples have\\nbeen removed.\\n??????\\tYou may shower before this time using a shower cap to cover\\nyour head.\\n??????\\tIncrease your intake of fluids and fiber, as narcotic pain\\nmedicine can cause constipation. We generally recommend taking\\nan over the counter stool softener, such as Docusate (Colace)\\nwhile taking narcotic pain medication.\\n??????\\tUnless directed by your doctor, do not take any\\nanti-inflammatory medicines such as Motrin, Aspirin, Advil, and\\nIbuprofen etc.\\n??????\\tClearance to drive and return to work will be addressed at\\nyour post-operative office visit.\\n\\nCALL YOUR SURGEON IMMEDIATELY IF YOU EXPERIENCE ANY OF THE\\nFOLLOWING\\n\\n??????\\tNew onset of tremors or seizures.\\n??????\\tAny confusion or change in mental status.\\n??????\\tAny numbness, tingling, weakness in your extremities.\\n??????\\tPain or headache that is continually increasing, or not\\nrelieved by pain medication.\\n??????\\tAny signs of infection at the wound site: redness, swelling,\\ntenderness, or drainage.\\n??????\\tFever greater than or equal to 101?????? F.\\n\\n\\nFollowup Instructions:\\nYou will need to see the nurse practitioner 14 days\\npost-operatively for suture removal. Please call [**Telephone/Fax (1) 1669**]\\nfor the appointment.\\nYou will need to follow up with Dr. [**Last Name (STitle) **] in 4 weeks with a\\nHead CT of the brain.\\n\\n\\n\\nCompleted by:[**2118-12-9**]']\n",
      "Notes processing done!!\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "regular_labels = []\n",
    "rolled_labels = []\n",
    "\n",
    "unique_categories = []\n",
    "texts_categories = []\n",
    "\n",
    "\n",
    "# tt = string.maketrans(string.digits, 'd' * len(string.digits))\n",
    "tt = str.maketrans(string.digits, 'd' * len(string.digits))\n",
    "# with open('notes.csv', 'rb') as f:\n",
    "count_row = 0\n",
    "with gzip.open(f'{DATA_DIR}NOTEEVENTS.csv.gz', 'rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        count_row = count_row + 1\n",
    "\n",
    "        key = (row[1], row[2])\n",
    "        cat = row[6]\n",
    "        if count_row == 10:\n",
    "            print(key, cat, row)\n",
    "            \n",
    "        if cat not in unique_categories:\n",
    "            unique_categories.append(cat)\n",
    "\n",
    "        if key in regular_diagnosis:\n",
    "            text = row[-1].strip().translate(tt)\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "                texts_categories.append(unique_categories.index(cat))\n",
    "                regular_labels.append(regular_diagnosis[key])\n",
    "                rolled_labels.append(rolled_diagnosis[key])\n",
    "print('Notes processing done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406190 406190 406190 406190\n"
     ]
    }
   ],
   "source": [
    "print(len(texts), len(texts_categories), len(regular_labels), len(rolled_labels))\n",
    "# 1851286 1851286 1851286 1851286\n",
    "# 406190 406190 406190 406190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average regular labels per report: 17.420187596937392\n",
      "Average rolled labels per report: 15.754735468623059\n"
     ]
    }
   ],
   "source": [
    "    print('Average regular labels per report:',\n",
    "          sum(map(len, regular_labels)) / len(regular_labels))\n",
    "    print('Average rolled labels per report:',\n",
    "          sum(map(len, rolled_labels)) / len(rolled_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens *before* preprocessing: 162715\n"
     ]
    }
   ],
   "source": [
    "  word_index = tokenizer.word_index\n",
    "  print('Unique tokens *before* preprocessing:', len(word_index))\n",
    "  # Unique tokens *before* preprocessing: 371809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Segment words on frequently/infrequently occuring\n",
    "  frequent_words = []\n",
    "  infrequent_words = []\n",
    "  for word, count in tokenizer.word_counts.items():\n",
    "      if count < MIN_WORD_OCCURENCES:\n",
    "          infrequent_words.append(word)\n",
    "      else:\n",
    "          frequent_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53229 109486\n"
     ]
    }
   ],
   "source": [
    "print(len(frequent_words), len(infrequent_words))\n",
    "# 109640 262169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_infrequent_word_mapping(infrequent_words, frequent_words):\n",
    "    print('infrequent_words len: ', len(infrequent_words))\n",
    "    print('frequent_words len: ', len(frequent_words))\n",
    "    if not os.path.exists(f'{DATA_DIR}infrequent_word_mapping.pickle'):\n",
    "        infrequent_word_mapping = {}\n",
    "        for idx, word in enumerate(infrequent_words):\n",
    "            if idx % 1000 == 0:\n",
    "                print('infrequent_words processed: ', idx)\n",
    "            dists = np.vectorize(lambda x: Levenshtein.distance(word, x))(frequent_words)\n",
    "            most_similar_word = frequent_words[np.argmin(dists)]\n",
    "            infrequent_word_mapping[word] = most_similar_word\n",
    "        with open(f'{DATA_DIR}infrequent_word_mapping.pickle', 'wb') as f:\n",
    "            pickle.dump(infrequent_word_mapping, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(f'{DATA_DIR}infrequent_word_mapping.pickle', 'rb') as f:\n",
    "            infrequent_word_mapping = pickle.load(f)\n",
    "    return infrequent_word_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infrequent_words len:  109486\n",
      "frequent_words len:  53229\n",
      "infrequent_words processed:  0\n",
      "infrequent_words processed:  1000\n",
      "infrequent_words processed:  2000\n",
      "infrequent_words processed:  3000\n",
      "infrequent_words processed:  4000\n",
      "infrequent_words processed:  5000\n",
      "infrequent_words processed:  6000\n",
      "infrequent_words processed:  7000\n",
      "infrequent_words processed:  8000\n",
      "infrequent_words processed:  9000\n",
      "infrequent_words processed:  10000\n",
      "infrequent_words processed:  11000\n",
      "infrequent_words processed:  12000\n",
      "infrequent_words processed:  13000\n",
      "infrequent_words processed:  14000\n",
      "infrequent_words processed:  15000\n",
      "infrequent_words processed:  16000\n",
      "infrequent_words processed:  17000\n",
      "infrequent_words processed:  18000\n",
      "infrequent_words processed:  19000\n",
      "infrequent_words processed:  20000\n",
      "infrequent_words processed:  21000\n",
      "infrequent_words processed:  22000\n",
      "infrequent_words processed:  23000\n",
      "infrequent_words processed:  24000\n",
      "infrequent_words processed:  25000\n",
      "infrequent_words processed:  26000\n",
      "infrequent_words processed:  27000\n",
      "infrequent_words processed:  28000\n",
      "infrequent_words processed:  29000\n",
      "infrequent_words processed:  30000\n",
      "infrequent_words processed:  31000\n",
      "infrequent_words processed:  32000\n",
      "infrequent_words processed:  33000\n",
      "infrequent_words processed:  34000\n",
      "infrequent_words processed:  35000\n",
      "infrequent_words processed:  36000\n",
      "infrequent_words processed:  37000\n",
      "infrequent_words processed:  38000\n",
      "infrequent_words processed:  39000\n",
      "infrequent_words processed:  40000\n",
      "infrequent_words processed:  41000\n",
      "infrequent_words processed:  42000\n",
      "infrequent_words processed:  43000\n",
      "infrequent_words processed:  44000\n",
      "infrequent_words processed:  45000\n",
      "infrequent_words processed:  46000\n",
      "infrequent_words processed:  47000\n",
      "infrequent_words processed:  48000\n",
      "infrequent_words processed:  49000\n",
      "infrequent_words processed:  50000\n",
      "infrequent_words processed:  51000\n",
      "infrequent_words processed:  52000\n",
      "infrequent_words processed:  53000\n",
      "infrequent_words processed:  54000\n",
      "infrequent_words processed:  55000\n",
      "infrequent_words processed:  56000\n",
      "infrequent_words processed:  57000\n",
      "infrequent_words processed:  58000\n",
      "infrequent_words processed:  59000\n",
      "infrequent_words processed:  60000\n",
      "infrequent_words processed:  61000\n",
      "infrequent_words processed:  62000\n",
      "infrequent_words processed:  63000\n",
      "infrequent_words processed:  64000\n",
      "infrequent_words processed:  65000\n",
      "infrequent_words processed:  66000\n",
      "infrequent_words processed:  67000\n",
      "infrequent_words processed:  68000\n",
      "infrequent_words processed:  69000\n",
      "infrequent_words processed:  70000\n",
      "infrequent_words processed:  71000\n",
      "infrequent_words processed:  72000\n",
      "infrequent_words processed:  73000\n",
      "infrequent_words processed:  74000\n",
      "infrequent_words processed:  75000\n",
      "infrequent_words processed:  76000\n",
      "infrequent_words processed:  77000\n",
      "infrequent_words processed:  78000\n",
      "infrequent_words processed:  79000\n",
      "infrequent_words processed:  80000\n",
      "infrequent_words processed:  81000\n",
      "infrequent_words processed:  82000\n",
      "infrequent_words processed:  83000\n",
      "infrequent_words processed:  84000\n",
      "infrequent_words processed:  85000\n",
      "infrequent_words processed:  86000\n",
      "infrequent_words processed:  87000\n",
      "infrequent_words processed:  88000\n",
      "infrequent_words processed:  89000\n",
      "infrequent_words processed:  90000\n",
      "infrequent_words processed:  91000\n",
      "infrequent_words processed:  92000\n",
      "infrequent_words processed:  93000\n",
      "infrequent_words processed:  94000\n",
      "infrequent_words processed:  95000\n",
      "infrequent_words processed:  96000\n",
      "infrequent_words processed:  97000\n",
      "infrequent_words processed:  98000\n",
      "infrequent_words processed:  99000\n",
      "infrequent_words processed:  100000\n",
      "infrequent_words processed:  101000\n",
      "infrequent_words processed:  102000\n",
      "infrequent_words processed:  103000\n",
      "infrequent_words processed:  104000\n",
      "infrequent_words processed:  105000\n",
      "infrequent_words processed:  106000\n",
      "infrequent_words processed:  107000\n",
      "infrequent_words processed:  108000\n",
      "infrequent_words processed:  109000\n"
     ]
    }
   ],
   "source": [
    "infrequent_word_mapping = generate_infrequent_word_mapping(infrequent_words,\n",
    "                                                               frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens *after* preprocessing: 53229\n"
     ]
    }
   ],
   "source": [
    "# Replace infrequent word with a frequent similar word\n",
    "infrequent_word_index = {}\n",
    "for word in infrequent_words:\n",
    "    most_similar_word = infrequent_word_mapping[word]\n",
    "    infrequent_word_index[word] = word_index[most_similar_word]\n",
    "    del word_index[word]\n",
    " \n",
    "print('Unique tokens *after* preprocessing:', len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimplementation of `tokenizer.texts_to_sequences`\n",
    "sequences = []\n",
    "for text in texts:\n",
    "    seq = text_to_word_sequence(text)\n",
    "    vec = []\n",
    "    for word in seq:\n",
    "        idx = word_index.get(word)\n",
    "        if idx is not None:\n",
    "            vec.append(idx)\n",
    "        else:\n",
    "            vec.append(infrequent_word_index[word])\n",
    "    sequences.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest sequence has 10 tokens\n",
      "Longest sequences has 2199 tokens\n",
      "Average tokens per sequence: 309.05638814582744\n"
     ]
    }
   ],
   "source": [
    "# Sequence must be < MAX_SEQ_LEN and > MIN_SEQ_LEN\n",
    "seqs = []\n",
    "cats = []\n",
    "reg_labels = []\n",
    "rol_labels = []\n",
    "for seq, cat, reg, rol in zip(sequences, texts_categories, regular_labels, rolled_labels):\n",
    "    if len(seq) < MAX_SEQ_LEN and len(seq) > MIN_SEQ_LEN:\n",
    "        seqs.append(seq)\n",
    "        cats.append(cat)\n",
    "        reg_labels.append(reg)\n",
    "        rol_labels.append(rol)\n",
    "sequences = seqs\n",
    "texts_categories = cats\n",
    "regular_labels = reg_labels\n",
    "rolled_labels = rol_labels\n",
    "\n",
    "lens = list(map(len, sequences))\n",
    "\n",
    "print('Shortest sequence has', min(lens), 'tokens')\n",
    "print('Longest sequences has', max(lens), 'tokens')\n",
    "print('Average tokens per sequence:', sum(lens) / len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{DATA_DIR}word_index.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenizer.word_index, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "texts = pad_sequences(sequences)\n",
    "\n",
    "# Encode labels as k-hot\n",
    "reg = np.zeros((len(regular_labels), len(regular_icd9_lookup)), dtype=np.int32)\n",
    "rol = np.zeros((len(rolled_labels), len(rolled_icd9_lookup)), dtype=np.int32)\n",
    "for i, label in enumerate(regular_labels): reg[i][label] = 1\n",
    "for i, label in enumerate(rolled_labels): rol[i][label] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_labels = reg\n",
    "rolled_labels = rol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "8 1\n",
      "24 1\n",
      "31 1\n",
      "38 1\n",
      "53 1\n",
      "61 1\n",
      "73 1\n",
      "99 1\n"
     ]
    }
   ],
   "source": [
    "for idx, lab in enumerate(rolled_labels[1]):\n",
    "  if lab == True:\n",
    "    print(idx, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts shape: (399623, 2199)\n",
      "Categories shape: (399623, 15)\n",
      "Regular labels shape: (399623, 4097)\n",
      "Rolled labels shape: (399623, 780)\n"
     ]
    }
   ],
   "source": [
    "# Encode categories as 1-hot\n",
    "cats = np.zeros((len(texts_categories), len(unique_categories)), dtype=np.float32)\n",
    "for i, cat in enumerate(texts_categories): cats[i][cat] = 1\n",
    "texts_categories = cats\n",
    "\n",
    "# keep labels with >= 1 examples\n",
    "regular_icd9_lookup = np.asarray(regular_icd9_lookup)\n",
    "rolled_icd9_lookup = np.asarray(rolled_icd9_lookup)\n",
    "\n",
    "keep = np.sum(regular_labels, 0) >= 1\n",
    "regular_labels = regular_labels[:, keep]\n",
    "regular_icd9_lookup = regular_icd9_lookup[keep]\n",
    "keep = np.sum(rolled_labels, 0) >= 1\n",
    "rolled_labels = rolled_labels[:, keep]\n",
    "rolled_icd9_lookup = rolled_icd9_lookup[keep]\n",
    "\n",
    "np.savez(f'{DATA_DIR}icd9_lookup.npz',\n",
    "          regular_icd9_lookup=regular_icd9_lookup,\n",
    "          rolled_icd9_lookup=rolled_icd9_lookup)\n",
    "\n",
    "print('Texts shape:', texts.shape)\n",
    "print('Categories shape:', texts_categories.shape)\n",
    "print('Regular labels shape:', regular_labels.shape)\n",
    "print('Rolled labels shape:', rolled_labels.shape)\n",
    "\n",
    "# Shuffle\n",
    "if os.path.exists(f'{DATA_DIR}shuffled_indices.npy'):\n",
    "    indices = np.load(f'{DATA_DIR}shuffled_indices.npy')\n",
    "else:\n",
    "    indices = np.arange(texts.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    np.save(f'{DATA_DIR}shuffled_indices.npy', indices)\n",
    "texts = texts[indices]\n",
    "texts_categories = texts_categories[indices]\n",
    "regular_labels = regular_labels[indices]\n",
    "rolled_labels = rolled_labels[indices]\n",
    "\n",
    "np.savez(f'{DATA_DIR}data.npz',\n",
    "          x=texts, cats=texts_categories,\n",
    "          reg_y=regular_labels, rol_y=rolled_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007 169\n",
      "2008 17\n",
      "2009 41\n",
      "2010 241\n",
      "2011 5429\n",
      "2012 223\n",
      "2013 2041\n",
      "2014 3\n",
      "2015 13\n",
      "2016 1458\n",
      "2017 3\n",
      "2018 3432\n",
      "2019 1859\n",
      "2020 13237\n",
      "2021 4709\n",
      "2022 104\n",
      "2023 3673\n",
      "2024 3\n",
      "2025 6233\n",
      "2026 223\n",
      "2027 155\n",
      "2028 3916\n",
      "2029 1593\n",
      "2030 9\n",
      "2031 77\n",
      "2032 12066\n",
      "2033 8578\n",
      "2034 11\n",
      "2035 6\n",
      "2036 389\n",
      "2037 389\n",
      "2038 17\n",
      "2039 1312\n",
      "2040 19\n",
      "2041 1\n",
      "2042 395\n",
      "2043 98\n",
      "2044 536\n",
      "2045 16503\n",
      "2046 658\n",
      "2047 837\n",
      "2048 100\n",
      "2049 3\n",
      "2050 1947\n",
      "2051 2215\n",
      "2052 2\n",
      "2053 1570\n",
      "2054 2233\n",
      "2055 567\n",
      "2056 4\n",
      "2057 5\n",
      "2058 1732\n",
      "2059 116\n",
      "2060 162\n",
      "2061 10\n",
      "2062 81\n",
      "2063 217\n",
      "2064 4\n",
      "2065 668\n",
      "2066 261\n",
      "2067 895\n",
      "2068 713\n",
      "2069 3\n",
      "2070 806\n",
      "2071 895\n",
      "2072 1\n",
      "2073 1278\n",
      "2074 26\n",
      "2075 16092\n",
      "2076 2\n",
      "2077 48\n",
      "2078 1\n",
      "2079 3237\n",
      "2080 2317\n",
      "2081 7\n",
      "2082 552\n",
      "2083 241\n",
      "2084 2\n",
      "2085 3417\n",
      "2086 7\n",
      "2087 2796\n",
      "2088 54\n",
      "2089 9008\n",
      "2090 13\n",
      "2091 295\n",
      "2092 949\n",
      "2093 7\n",
      "2094 789\n",
      "2095 156\n",
      "2096 1402\n",
      "2097 248\n",
      "2098 4\n",
      "2099 1629\n",
      "2100 399\n",
      "2101 530\n",
      "2102 2406\n",
      "2103 9\n",
      "2104 15429\n",
      "2105 248\n",
      "2106 1460\n",
      "2107 1072\n",
      "2108 9\n",
      "2109 286\n",
      "2110 491\n",
      "2111 745\n",
      "2112 121\n",
      "2113 56\n",
      "2114 396\n",
      "2115 19\n",
      "2116 521\n",
      "2117 197\n",
      "2118 13\n",
      "2119 206\n",
      "2120 13\n",
      "2121 2237\n",
      "2122 35\n",
      "2123 923\n",
      "2124 478\n",
      "2125 1287\n",
      "2126 4\n",
      "2127 1349\n",
      "2128 681\n",
      "2129 121\n",
      "2130 1961\n",
      "2131 82\n",
      "2132 11\n",
      "2133 235\n",
      "2134 571\n",
      "2135 425\n",
      "2136 15007\n",
      "2137 4\n",
      "2138 2613\n",
      "2139 681\n",
      "2140 121\n",
      "2141 433\n",
      "2142 95\n",
      "2143 11\n",
      "2144 286\n",
      "2145 11829\n",
      "2146 744\n",
      "2147 206\n",
      "2148 751\n",
      "2149 13\n",
      "2150 969\n",
      "2151 19\n",
      "2152 28\n",
      "2153 123\n",
      "2154 36\n",
      "2155 314\n",
      "2156 4\n",
      "2157 1629\n",
      "2158 314\n",
      "2159 1732\n",
      "2160 3\n",
      "2161 2233\n",
      "2162 4\n",
      "2163 155\n",
      "2164 200\n",
      "2165 392\n",
      "2166 589\n",
      "2167 697\n",
      "2168 3\n",
      "2169 480\n",
      "2170 22\n",
      "2171 1674\n",
      "2172 2\n",
      "2173 37\n",
      "2174 1366\n",
      "2175 3\n",
      "2176 8077\n",
      "2177 505\n",
      "2178 4\n",
      "2179 64\n",
      "2180 1094\n",
      "2181 23\n",
      "2182 1\n",
      "2183 2\n",
      "2184 1090\n",
      "2185 118\n",
      "2186 154\n",
      "2187 1674\n",
      "2188 160\n",
      "2189 114\n",
      "2190 3\n",
      "2191 1509\n",
      "2192 3\n",
      "2193 1373\n",
      "2194 22\n",
      "2195 1674\n",
      "2196 3\n",
      "2197 22\n",
      "2198 454\n"
     ]
    }
   ],
   "source": [
    "for idx, txt in enumerate(texts[0]):\n",
    "  if txt != 0:\n",
    "    print(idx, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1\n",
      "7 1\n",
      "8 1\n",
      "14 1\n",
      "18 1\n",
      "27 1\n",
      "31 1\n",
      "33 1\n",
      "38 1\n",
      "44 1\n",
      "71 1\n",
      "72 1\n",
      "73 1\n",
      "78 1\n",
      "93 1\n",
      "94 1\n",
      "98 1\n",
      "100 1\n",
      "128 1\n",
      "200 1\n",
      "326 1\n",
      "498 1\n"
     ]
    }
   ],
   "source": [
    "for idx, lab in enumerate(rolled_labels[1]):\n",
    "  if lab == True:\n",
    "    print(idx, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d114184c886ba29bffe38c6c6fdadc7ffef7ccc1b8a3a158ab752daf223c4cdc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cs598')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
