{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import gzip\n",
    "\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "random.seed(3778)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_diag_diabetes_hadm_ids.shape:    (14222,)\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h1/bbx29rgs7297rs64yfyxr0kh0000gn/T/ipykernel_60427/4244106981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_diag_diabetes_hadm_ids.shape:   '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_diag_diabetes_hadm_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_diag_icd9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_DIR}diag_icd9.csv.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_diag_icd9.shape:                '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_diag_icd9\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs598/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# load data from saved files - preprocessed data for notes and diagnosis codes\n",
    "\n",
    "df_diag_diabetes_hadm_ids = pkl.load(open(f'{DATA_DIR}diag_diabetes_hadm_ids.p','rb'))\n",
    "print('df_diag_diabetes_hadm_ids.shape:   ', df_diag_diabetes_hadm_ids.shape)\n",
    "\n",
    "df_diag_icd9 = (pd.read_csv(f'{DATA_DIR}diag_icd9.csv.gz'))\n",
    "print('df_diag_icd9.shape:                ', df_diag_icd9.shape)\n",
    "\n",
    "df_diag_icd9_rolled = (pd.read_csv(f'{DATA_DIR}diag_icd9_rolled.csv.gz', 'rb', low_memory=False))\n",
    "print('df_diag_icd9_rolled.shape:         ', df_diag_icd9_rolled.shape)\n",
    "\n",
    "df_notes = (pd.read_csv(f'{DATA_DIR}notes_final.csv.gz', low_memory=False))\n",
    "print('df_notes.shape:                    ', df_notes.shape)\n",
    "\n",
    "notes_tokens_list = pkl.load(open(f'{DATA_DIR}notes_tokens_list.p','rb'))\n",
    "print('notes_tokens_list.length:          ', len(notes_tokens_list))\n",
    "\n",
    "icd9_unique_list = pkl.load(open(f'{DATA_DIR}diag_icd9_unique_list.p','rb'))\n",
    "print('icd9_unique_list.len:              ', len(icd9_unique_list))\n",
    "\n",
    "icd9_rolled_unique_list = pkl.load(open(f'{DATA_DIR}diag_icd9_rolled_unique_list.p','rb'))\n",
    "print('icd9_rolled_unique_list.len:       ', len(icd9_rolled_unique_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of used records              399631\n",
      "Num. of regular labels            4103\n",
      "Num. of rolled up labels          781\n",
      "Num. of unique tokens             53229\n",
      "Avg. num. of tokens per report    309.09424193818796\n"
     ]
    }
   ],
   "source": [
    "# Statistics for comparison with original paper statistics after preprocessing\n",
    "\n",
    "print('Num. of used records             ', df_notes.shape[0])\n",
    "print('Num. of regular labels           ',len(icd9_unique_list))\n",
    "print('Num. of rolled up labels         ',len(icd9_rolled_unique_list))\n",
    "print('Num. of unique tokens            ',len(notes_tokens_list))\n",
    "print('Avg. num. of tokens per report   ',df_notes['NTOKENS_LEN'].sum() / len(df_notes['NTOKENS_LEN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_list len:  399631\n"
     ]
    }
   ],
   "source": [
    "# list of all notes\n",
    "text_list = df_notes['NTOKENS'].to_list()\n",
    "print('text_list len: ', len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_notes_icd9.shape        :  (399631, 6)\n",
      "df_notes_icd9_rolled.shape :  (399631, 6)\n"
     ]
    }
   ],
   "source": [
    "df_notes_icd9 = pd.merge(df_notes, df_diag_icd9, on=['HADM_ID'], how='inner').drop(columns = ['TEXT', 'TOKENS', 'SUBJECT_ID', 'NTOKENS_LEN'])\n",
    "df_notes_icd9_rolled = pd.merge(df_notes, df_diag_icd9_rolled, on=['HADM_ID'], how='inner').drop(columns = ['TEXT', 'TOKENS', 'SUBJECT_ID', 'NTOKENS_LEN'])\n",
    "print('df_notes_icd9.shape        : ', df_notes_icd9.shape)\n",
    "print('df_notes_icd9_rolled.shape : ', df_notes_icd9_rolled.shape)\n",
    "\n",
    "# print(df_notes_icd9.head(2))\n",
    "# print(df_notes_icd9_rolled.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read from saved model file:  Word2Vec(vocab=53203, vector_size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "w2v_model_file = f'{DATA_DIR}word2vec_model.model'\n",
    "\n",
    "if os.path.exists(w2v_model_file):\n",
    "  # read from saved file\n",
    "  model_w2v = Word2Vec.load(w2v_model_file)\n",
    "  print('read from saved model file: ', model_w2v)\n",
    "else:\n",
    "  # initialize Word2Vec\n",
    "  model_w2v = Word2Vec(min_count=1, vector_size=300, workers=4, sg=1, seed=3778)\n",
    "  print('model initialized: ',model_w2v)\n",
    "\n",
    "  model_w2v.build_vocab(text_list)\n",
    "  print('model vacab created: ',model_w2v)\n",
    "\n",
    "  # list of words not in word2vec vocab\n",
    "  dict1 = model_w2v.wv.index_to_key\n",
    "  dict2 = notes_tokens_list.keys()\n",
    "  print('tokens not in w2v vocab: ', dict2 - dict1 )\n",
    "\n",
    "  # Word2Vec model training - trained model saved\n",
    "  model_w2v.train(text_list, \n",
    "                  total_examples=model_w2v.corpus_count, \n",
    "                  epochs=model_w2v.epochs)\n",
    "  # Write to file \n",
    "  model_w2v.save(w2v_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix.shape:  (53203, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create word embedding matrix\n",
    "embedding_matrix = model_w2v.wv[model_w2v.wv.index_to_key]\n",
    "print('embedding_matrix.shape: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_dict.length:  53203\n",
      "row_dict.length after padding:  53205\n"
     ]
    }
   ],
   "source": [
    "# Create dict for embedding matrix (word <-> row)\n",
    "row_dict=dict({word:idx for idx,word in enumerate(model_w2v.wv.index_to_key)})\n",
    "print('row_dict.length: ', len(row_dict))\n",
    "# Create and map unknown and padding tokens to null\n",
    "embedding_matrix = np.concatenate((embedding_matrix, np.zeros((2,300))), axis=0)\n",
    "row_dict['_unknown_'] = len(model_w2v.wv.index_to_key)\n",
    "row_dict['_padding_'] = len(model_w2v.wv.index_to_key) + 1\n",
    "print('row_dict.length after padding: ', len(row_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_dict.length after padding:  53205\n",
      "embedding_matrix.shape:  (53203, 300)\n"
     ]
    }
   ],
   "source": [
    "# save embedded matrix and row_dict to file\n",
    "# pkl.dump(row_dict, open(f'{DATA_DIR}row_index_dictionary.p', 'wb'))\n",
    "# pkl.dump(embedding_matrix, open(f'{DATA_DIR}embedded_matrix.p', 'wb'))\n",
    "\n",
    "# read from saved file\n",
    "row_dict = pkl.load(open(f'{DATA_DIR}row_index_dictionary.p', 'rb'))\n",
    "embedding_matrix = pkl.load(open(f'{DATA_DIR}embedded_matrix.p', 'rb'))\n",
    "print('row_dict.length after padding: ', len(row_dict))\n",
    "print('embedding_matrix.shape: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_token_to_index(tokens, row_dict):\n",
    "    return [row_dict.get(token, row_dict['_unknown_']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 2200\n",
    "\n",
    "indexed_notes = (df_notes['NTOKENS']\n",
    "      .apply(convert_token_to_index, row_dict=row_dict)\n",
    "      .apply(lambda x: np.squeeze(pad_sequences([x], padding = 'post', truncating = 'post', \n",
    "      maxlen = MAX_LENGTH, value = row_dict['_padding_']))))\n",
    "      \n",
    "print(type(indexed_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.type:  <class 'numpy.ndarray'>\n",
      "X.shape:  (399631, 2200)\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack(indexed_notes.to_list())\n",
    "print('X.type: ',type(X))\n",
    "print('X.shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_notes_icd9.shape:  (399631, 7)\n"
     ]
    }
   ],
   "source": [
    "df_notes_icd9['INDEXED_TOKENS'] = [x for x in X]\n",
    "print('df_notes_icd9.shape: ', df_notes_icd9.shape)\n",
    "# print(df_notes_icd9.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_notes_icd9_rolled.shape:  (399631, 7)\n"
     ]
    }
   ],
   "source": [
    "df_notes_icd9_rolled['INDEXED_TOKENS'] = [x for x in X]\n",
    "print('df_notes_icd9_rolled.shape: ', df_notes_icd9_rolled.shape)\n",
    "# print(df_notes_icd9_rolled.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200 - 428 =  1772 , 2200 - 306 =  1894\n",
      "2200 - 428 =  1772 , 2200 - 306 =  1894\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "t1 = dict(Counter(df_notes_icd9['INDEXED_TOKENS'][0]))\n",
    "t2 = dict(Counter(df_notes_icd9['INDEXED_TOKENS'][1]))\n",
    "print('2200 - 428 = ', t1[53204],',', '2200 - 306 = ', t2[53204])\n",
    "\n",
    "t1 = dict(Counter(df_notes_icd9_rolled['INDEXED_TOKENS'][0]))\n",
    "t2 = dict(Counter(df_notes_icd9_rolled['INDEXED_TOKENS'][1]))\n",
    "print('2200 - 428 = ', t1[53204],',', '2200 - 306 = ', t2[53204])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_ICD9.type:  <class 'numpy.ndarray'>\n",
      "Y_ICD9.shape:  (399631, 4103)\n"
     ]
    }
   ],
   "source": [
    "Y_ICD9 = np.vstack(df_notes_icd9['ICD9_CODE_MLB'].to_numpy())\n",
    "print('Y_ICD9.type: ',type(Y_ICD9))\n",
    "print('Y_ICD9.shape: ', Y_ICD9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_ICD9_ROLLED.type:  <class 'numpy.ndarray'>\n",
      "Y_ICD9_ROLLED.shape:  (399631, 781)\n"
     ]
    }
   ],
   "source": [
    "Y_ICD9_ROLLED = np.vstack(df_notes_icd9_rolled['ICD9_CODE_ROLLED_MLB'].to_numpy())\n",
    "print('Y_ICD9_ROLLED.type: ',type(Y_ICD9_ROLLED))\n",
    "print('Y_ICD9_ROLLED.shape: ', Y_ICD9_ROLLED.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from saved file Y_ICD9_ROLLED_FILE:  ../data/Y_ICD9_ROLLED.gz\n",
      "Y_ICD9_ROLLED.type:  <class 'numpy.ndarray'>\n",
      "Y_ICD9_ROLLED.shape:  (399631, 781)\n"
     ]
    }
   ],
   "source": [
    "X_NOTES_INDEXED_FILE = f'{DATA_DIR}X_NOTES_INDEXED.gz'\n",
    "Y_ICD9_FILE = f'{DATA_DIR}Y_ICD9.gz'\n",
    "Y_ICD9_ROLLED_FILE = f'{DATA_DIR}Y_ICD9_ROLLED.gz'\n",
    "\n",
    "# X dataset\n",
    "if os.path.exists(X_NOTES_INDEXED_FILE):\n",
    "  print ('reading from saved file X_NOTES_INDEXED_FILE: ', X_NOTES_INDEXED_FILE)\n",
    "  with gzip.open(X_NOTES_INDEXED_FILE, \"rb\") as f:\n",
    "      X = pkl.load(f)\n",
    "  print('X.type: ',type(X))\n",
    "  print('X.shape: ', X.shape)\n",
    "else:\n",
    "  # save data and label to file\n",
    "  with gzip.open(X_NOTES_INDEXED_FILE, \"wb\") as f:\n",
    "      pkl.dump(X, f)\n",
    "  print('X saved')\n",
    "\n",
    "# Y_ICD9 dataset\n",
    "if os.path.exists(Y_ICD9_FILE):\n",
    "  print ('reading from saved file Y_ICD9_FILE: ', Y_ICD9_FILE)\n",
    "  with gzip.open(Y_ICD9_FILE, \"rb\") as f:\n",
    "      Y_ICD9 = pkl.load(f)\n",
    "  print('Y_ICD9.type: ',type(Y_ICD9))\n",
    "  print('Y_ICD9.shape: ', Y_ICD9.shape)\n",
    "else:\n",
    "  # save data and label to file\n",
    "  with gzip.open(Y_ICD9_FILE, \"wb\") as f:\n",
    "      pkl.dump(Y_ICD9, f)\n",
    "  print('Y_ICD9 saved')\n",
    "\n",
    "# Y_ICD9_ROLLED dataset\n",
    "if os.path.exists(Y_ICD9_ROLLED_FILE):\n",
    "  print ('reading from saved file Y_ICD9_ROLLED_FILE: ', Y_ICD9_ROLLED_FILE)\n",
    "  with gzip.open(Y_ICD9_ROLLED_FILE, \"rb\") as f:\n",
    "      Y_ICD9_ROLLED = pkl.load(f)\n",
    "  print('Y_ICD9_ROLLED.type: ',type(Y_ICD9_ROLLED))\n",
    "  print('Y_ICD9_ROLLED.shape: ', Y_ICD9_ROLLED.shape)\n",
    "else:\n",
    "  # save data and label to file\n",
    "  with gzip.open(Y_ICD9_ROLLED_FILE, \"wb\") as f:\n",
    "      pkl.dump(Y_ICD9_ROLLED, f)\n",
    "  print('Y_ICD9_ROLLED saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d114184c886ba29bffe38c6c6fdadc7ffef7ccc1b8a3a158ab752daf223c4cdc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cs598')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
