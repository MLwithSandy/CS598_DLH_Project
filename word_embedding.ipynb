{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import gzip\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "random.seed(3778)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_diag_diabetes_hadm_ids.shape:    (14222,)\n",
      "df_diag_icd9.shape:                 (14222, 3)\n",
      "df_diag_icd9_rolled.shape:          (14222, 3)\n",
      "df_notes.shape:                     (399623, 7)\n",
      "notes_tokens_list.length:           53229\n",
      "icd9_unique_list.len:               4103\n",
      "icd9_rolled_unique_list.len:        781\n"
     ]
    }
   ],
   "source": [
    "# load data from saved files - preprocessed data for notes and diagnosis codes\n",
    "\n",
    "df_diag_diabetes_hadm_ids = pkl.load(open(f'{DATA_DIR}diag_diabetes_hadm_ids.p','rb'))\n",
    "print('df_diag_diabetes_hadm_ids.shape:   ', df_diag_diabetes_hadm_ids.shape)\n",
    "\n",
    "with gzip.open(f'{DATA_DIR}diag_icd9.csv.gz', \"rb\") as f:\n",
    "    df_diag_icd9 = pkl.load(f)\n",
    "print('df_diag_icd9.shape:                ', df_diag_icd9.shape)\n",
    "\n",
    "with gzip.open(f'{DATA_DIR}diag_icd9_rolled.csv.gz', \"rb\") as f:\n",
    "    df_diag_icd9_rolled = pkl.load(f)\n",
    "print('df_diag_icd9_rolled.shape:         ', df_diag_icd9_rolled.shape)\n",
    "\n",
    "with gzip.open(f'{DATA_DIR}notes_final.gz', \"rb\") as f:\n",
    "    df_notes = pkl.load(f)\n",
    "print('df_notes.shape:                    ', df_notes.shape)\n",
    "\n",
    "notes_tokens_list = pkl.load(open(f'{DATA_DIR}notes_tokens_list.p','rb'))\n",
    "print('notes_tokens_list.length:          ', len(notes_tokens_list))\n",
    "\n",
    "icd9_unique_list = pkl.load(open(f'{DATA_DIR}diag_icd9_unique_list.p','rb'))\n",
    "print('icd9_unique_list.len:              ', len(icd9_unique_list))\n",
    "\n",
    "icd9_rolled_unique_list = pkl.load(open(f'{DATA_DIR}diag_icd9_rolled_unique_list.p','rb'))\n",
    "print('icd9_rolled_unique_list.len:       ', len(icd9_rolled_unique_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_notes['NTOKENS'][0]) == df_notes['NTOKENS_LEN'][0], 'length of 1st sequences does not match, incorrect data'\n",
    "assert len(df_notes['NTOKENS'][1]) == df_notes['NTOKENS_LEN'][1], 'length of 2nd sequences does not match, incorrect data'\n",
    "assert len(df_notes['NTOKENS'][2]) == df_notes['NTOKENS_LEN'][2], 'length of 3rd sequences does not match, incorrect data'\n",
    "assert len(df_notes['NTOKENS'][399622]) == df_notes['NTOKENS_LEN'][399622], 'length of 399622 sequences does not match, incorrect data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of used records              399623\n",
      "Num. of regular labels            4103\n",
      "Num. of rolled up labels          781\n",
      "Num. of unique tokens             53229\n",
      "Avg. num. of tokens per report    309.05638814582744\n"
     ]
    }
   ],
   "source": [
    "# Statistics for comparison with original paper statistics after preprocessing\n",
    "\n",
    "print('Num. of used records             ', df_notes.shape[0])\n",
    "print('Num. of regular labels           ',len(icd9_unique_list))\n",
    "print('Num. of rolled up labels         ',len(icd9_rolled_unique_list))\n",
    "print('Num. of unique tokens            ',len(notes_tokens_list))\n",
    "print('Avg. num. of tokens per report   ',df_notes['NTOKENS_LEN'].sum() / len(df_notes['NTOKENS_LEN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_notes_icd9.shape        :  (399623, 9)\n",
      "df_notes_icd9_rolled.shape :  (399623, 9)\n",
      "Average regular labels per report: 17.409070548992425\n",
      "Average rolled labels per report: 15.746078178683408\n"
     ]
    }
   ],
   "source": [
    "df_notes_icd9 = pd.merge(df_notes, df_diag_icd9, on=['HADM_ID'], how='inner').drop(columns = ['NTOKENS_LEN'])\n",
    "df_notes_icd9['ICD9_CODE_LEN'] = df_notes_icd9['ICD9_CODE'].apply(len)\n",
    "\n",
    "df_notes_icd9_rolled = pd.merge(df_notes, df_diag_icd9_rolled, on=['HADM_ID'], how='inner').drop(columns = [ 'NTOKENS_LEN'])\n",
    "df_notes_icd9_rolled['ICD9_CODE_ROLLED_LEN'] = df_notes_icd9_rolled['ICD9_CODE_ROLLED'].apply(len)\n",
    "print('df_notes_icd9.shape        : ', df_notes_icd9.shape)\n",
    "print('df_notes_icd9_rolled.shape : ', df_notes_icd9_rolled.shape)\n",
    "\n",
    "# print(df_notes_icd9.head(2))\n",
    "# print(df_notes_icd9_rolled.head(2))\n",
    "\n",
    "print('Average regular labels per report:', df_notes_icd9['ICD9_CODE_LEN'].sum()/len(df_notes_icd9['ICD9_CODE_LEN']))\n",
    "print('Average rolled labels per report:', df_notes_icd9_rolled['ICD9_CODE_ROLLED_LEN'].sum()/len(df_notes_icd9_rolled['ICD9_CODE_ROLLED_LEN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all notes\n",
    "text_list = df_notes['NTOKENS'].to_list()\n",
    "\n",
    "assert len(text_list[0]) == df_notes['NTOKENS_LEN'][0], 'length of 1st sequences does not match, incorrect data'\n",
    "assert len(text_list[1]) == df_notes['NTOKENS_LEN'][1], 'length of 2nd sequences does not match, incorrect data'\n",
    "assert len(text_list[2]) == df_notes['NTOKENS_LEN'][2], 'length of 3rd sequences does not match, incorrect data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h1/bbx29rgs7297rs64yfyxr0kh0000gn/T/ipykernel_83807/3614377263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model trained'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mmodel_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "w2v_model_file = f'{DATA_DIR}word2vec_model.model'\n",
    "word2vec_file = f'{DATA_DIR}word2vec.vec'\n",
    "\n",
    "if os.path.exists(w2v_model_file):\n",
    "  # read from saved file\n",
    "  model_w2v = Word2Vec.load(w2v_model_file)\n",
    "  print('read from saved model file: ', model_w2v)\n",
    "else:\n",
    "  # train Word2Vec\n",
    "  model_w2v = Word2Vec(text_list, vector_size=300, min_count=1,workers=4)\n",
    "  print('model trained')\n",
    "  model_w2v.save(w2v_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "tokens not in w2v vocab:  26\n",
      "missing words in vocab:  [('cefdinir', 7), ('reinnervation', 10), ('apoplexy', 5), ('burgdorferi', 6), ('fidaxomicin', 5), ('oxymorphone', 8), ('megakaryocytic', 6), ('larygneal', 5), ('granulicatella', 5), ('gdnxl', 14), ('foliaceus', 10), ('ifgdnxl', 22), ('arrangment', 5), ('ampliprep', 6), ('dasatinib', 10), ('itsfq', 21), ('hnxcjojqj', 23), ('quantitaton', 5), ('subcutis', 5), ('petites', 7), ('fditsfq', 13), ('holdblu', 10), ('barrx', 6), ('discontinuance', 5), ('rivaroxaban', 5), ('hepatocholangiolar', 5)]\n"
     ]
    }
   ],
   "source": [
    "  model_w2v.wv.save_word2vec_format(word2vec_file, binary=False)\n",
    "  print('model saved')\n",
    "\n",
    "  # list of words not in word2vec vocab\n",
    "  dict1 = model_w2v.wv.index_to_key\n",
    "  dict2 = notes_tokens_list.keys()\n",
    "\n",
    "  missing_word_in_w2v_vocab = []\n",
    "  print('tokens not in w2v vocab: ', len(dict2 - dict1))\n",
    "  for x in (dict2 - dict1):\n",
    "    missing_word_in_w2v_vocab.append((x, notes_tokens_list[x])) \n",
    "  \n",
    "  print('missing words in vocab: ', missing_word_in_w2v_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=53203, vector_size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix.shape:  (53203, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create word embedding matrix\n",
    "embedding_matrix = model_w2v.wv[model_w2v.wv.index_to_key]\n",
    "print('embedding_matrix.shape: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_dict.length:  53203\n",
      "row_dict.length after padding:  53205\n"
     ]
    }
   ],
   "source": [
    "# Create dict for embedding matrix (word <-> row)\n",
    "row_dict=dict({word:idx + 1 for idx,word in enumerate(model_w2v.wv.index_to_key)})\n",
    "print('row_dict.length: ', len(row_dict))\n",
    "# Create and map unknown and padding tokens to null\n",
    "embedding_matrix = np.concatenate((embedding_matrix, np.zeros((2,300))), axis=0)\n",
    "row_dict['_unknown_'] = len(model_w2v.wv.index_to_key) + 1\n",
    "row_dict['_padding_'] = 0\n",
    "print('row_dict.length after padding: ', len(row_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading row_dict from file:  ../data/row_index_dictionary.p\n",
      "row_dict.length after padding:  53205\n",
      "reading embedding_matrix from file:  ../data/embedding_matrix.p\n",
      "embedding_matrix.shape:  (53205, 300)\n"
     ]
    }
   ],
   "source": [
    "row_index_dictionary_file = f'{DATA_DIR}row_index_dictionary.p'\n",
    "embedding_matrix_file = f'{DATA_DIR}embedding_matrix.p'\n",
    "\n",
    "# save embedded matrix and row_dict to file\n",
    "if not os.path.exists(row_index_dictionary_file):\n",
    "  pkl.dump(row_dict, open(row_index_dictionary_file, 'wb'))\n",
    "else:\n",
    "  # read from saved file\n",
    "  print('reading row_dict from file: ', row_index_dictionary_file)\n",
    "  row_dict = pkl.load(open(row_index_dictionary_file, 'rb'))\n",
    "  print('row_dict.length after padding: ', len(row_dict))\n",
    "\n",
    "if not os.path.exists(embedding_matrix_file):\n",
    "  pkl.dump(embedding_matrix, open(embedding_matrix_file, 'wb'))\n",
    "else:\n",
    "  # read from saved file\n",
    "  print('reading embedding_matrix from file: ', embedding_matrix_file)\n",
    "  embedding_matrix = pkl.load(open(embedding_matrix_file, 'rb'))\n",
    "  print('embedding_matrix.shape: ', embedding_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_token_to_index(tokens, row_dict):\n",
    "    return [row_dict.get(token, row_dict['_unknown_']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 2199\n",
    "\n",
    "indexed_notes = (df_notes['NTOKENS']\n",
    "      .apply(convert_token_to_index, row_dict=row_dict)\n",
    "      .apply(lambda x: np.squeeze(pad_sequences([x], padding = 'pre', truncating = 'post', \n",
    "      maxlen = MAX_LENGTH, value = row_dict['_padding_']))))\n",
    "      \n",
    "print(type(indexed_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.type:  <class 'numpy.ndarray'>\n",
      "X.shape:  (399623, 2199)\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack(indexed_notes.to_list())\n",
    "print('X.type: ',type(X))\n",
    "print('X.shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_ICD9.type:  <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.int64'>\n",
      "Y_ICD9.shape:  (399623, 4103)\n"
     ]
    }
   ],
   "source": [
    "Y_ICD9 = np.vstack(df_notes_icd9['ICD9_CODE_MLB'].to_numpy())\n",
    "print('Y_ICD9.type: ',type(Y_ICD9), type(Y_ICD9[0]), type(Y_ICD9[0][0]))\n",
    "print('Y_ICD9.shape: ', Y_ICD9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_ICD9_ROLLED.type:  <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.int64'>\n",
      "Y_ICD9_ROLLED.shape:  (399623, 781)\n"
     ]
    }
   ],
   "source": [
    "Y_ICD9_ROLLED = np.vstack(df_notes_icd9_rolled['ICD9_CODE_ROLLED_MLB'].to_numpy())\n",
    "print('Y_ICD9_ROLLED.type: ',type(Y_ICD9_ROLLED), type(Y_ICD9_ROLLED[0]), type(Y_ICD9_ROLLED[0][0]))\n",
    "print('Y_ICD9_ROLLED.shape: ', Y_ICD9_ROLLED.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_categories len:  15 ['Case Management ' 'Consult' 'Discharge summary' 'ECG' 'Echo' 'General'\n",
      " 'Nursing' 'Nursing/other' 'Nutrition' 'Pharmacy' 'Physician ' 'Radiology'\n",
      " 'Rehab Services' 'Respiratory ' 'Social Work']\n"
     ]
    }
   ],
   "source": [
    "unique_categories = df_notes['CATEGORY'].unique()\n",
    "unique_categories.sort()\n",
    "\n",
    "print('unique_categories len: ', len(unique_categories), unique_categories)\n",
    "\n",
    "cats = np.zeros((len(df_notes['CATEGORY']), len(unique_categories)), dtype=np.float32)\n",
    "for i, cat in enumerate(df_notes['CATEGORY']):\n",
    "  cats[i][np.where(unique_categories == cat)] = 1\n",
    "\n",
    "notes_categories = cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read from indices file\n",
      "399623 62930\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices_file = f'{DATA_DIR}shuffled_indices.npy'\n",
    "\n",
    "# Shuffle\n",
    "if os.path.exists(shuffled_indices_file):\n",
    "    print('read from indices file', )\n",
    "    indices = np.load(shuffled_indices_file)\n",
    "else:\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    np.save(shuffled_indices_file, indices)\n",
    "\n",
    "print (len(indices), indices[5])\n",
    "\n",
    "X_SH = X[indices]\n",
    "CAT_SH = notes_categories[indices]\n",
    "Y_ICD9_SH = Y_ICD9[indices]\n",
    "Y_ICD9_ROLLED_SH = Y_ICD9_ROLLED[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from save data file:  ../data/data.npz\n",
      "(399623, 2199) (399623, 15) (399623, 4103) (399623, 781)\n"
     ]
    }
   ],
   "source": [
    "data_file = f'{DATA_DIR}data.npz'\n",
    "\n",
    "if os.path.exists(data_file):\n",
    "  print('loading from save data file: ', data_file)\n",
    "  data = np.load(f'{DATA_DIR}data.npz')\n",
    "  X_SH = data['x']\n",
    "  CAT_SH = data['cats']\n",
    "  Y_ICD9_SH = data['reg_y']\n",
    "  Y_ICD9_ROLLED_SH = data['rol_y']\n",
    "  print(X_SH.shape, CAT_SH.shape, Y_ICD9_SH.shape, Y_ICD9_ROLLED_SH.shape)\n",
    "else:\n",
    "  np.savez_compressed(f'{DATA_DIR}data.npz',\n",
    "            x=X_SH, cats=CAT_SH,\n",
    "            reg_y=Y_ICD9_SH, rol_y=Y_ICD9_ROLLED_SH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d114184c886ba29bffe38c6c6fdadc7ffef7ccc1b8a3a158ab752daf223c4cdc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cs598')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
